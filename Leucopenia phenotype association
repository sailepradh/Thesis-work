Part-1) Quality Control of the Variants and samples

First using the raw.vcf files we, we first used the filtration of on the basis of the genotyping rate and identity by descent to remove the five samples from the study cohort. On the study cohort we used the following filtration pipelines such that final sample number in the cohort in 211. we used the follwing filtration pipelines in the vcf tools.

Original Variants in the VCF files =  211467

1) First, we used the filtration criteria based on the variant quality. We only considered using the variants sites with mean depth values (over all included individuals) greater than or equal to 10 and those variants which have passed the GATK filteration pipeline. 
  Parameter used 
  --vcf raw.vcf
	--min-meanDP 10
	--out Filter1
	--recode
	--remove-filtered-all
	
	The number of variants= 158753 

2) Subsequently on the above file we again used the quality criteria such that based on genotyping rate and duplicates result from IBD clustering. We removed the samples S0328 ,S0664, S0580, S0724, S0922 based on the lowergenotyping rate and exceptionally high variants count in the sample.
--vcf Filter1.recode.vcf
	--exclude sample_remove
	--max-missing 0.95
	--out Common_rare
	--recode
	
	The number of the variants = 152303 
	
On the subsequent files we obtained we defined the minor allele frquency threshold as 0.01 and divided the file into Common and rare variants based on greater or less than the threshold.

Part 2) Leucopenia data Analysis.
## Updated phenotype excel sheet was provided by benjamin. This new phenotype information contains the baseline leucopenia values of the three phenotypes. Analysis done in R statistical software

A) Normalization of the phenotype values:
Firstly, we considered the normalization of the leucopenia values in the sample. Considering benzamin idea of the measuring the difference in the leucopenia decrease from the intial baseline leucopenia values. We first calculated the decrease in the lecuopenia values using the formula,

Leu_decrease = (Leu_Baseline-Leu_Nadir)/Leu_Baseline = (1-(Leu_nadir/Leu_Baseline))

And subsequenly we normalized the values using the package multic package and Empirical Normal Quantile transformation (ENQT). For the time being, we used the logtransformed Nadir_leucopenia, ENQT Nadir Values, Leu_Decrease and ENQT Leu_Decrease values. And observing the basic single nucleotide association studies. Upon doing some analysis in the R I found out that the phenotypes are not homogenoous as expected from the prior knowldege. There are certain specifically important samples that could provide us with more information than the other samples. From the above manipulation we got the aggergrate phenotype file which contains the log normalized, EQRT normalized and ENQT_Decrease values which are then associated with the genotype data from exome sequenicing.

B) Using plink tools to associate the Common variants(>0.01) with the individual phenotype of interest
Creating the MAP and PED files that are read by the plink

vcftools --vcf Common.recode.vcf --plink --out Common

Doing the allelic association with the files. First associating with the normal allelic with the log normalized values using the parameters
	--file Common
	--pheno ../Leucopenia.fam
	--mpheno 4
	--linear
	--out Leu_log_allelic
	--noweb
	--adjust
	
	
Upon doing the association study we found that the using the lognormalized phwnotype values does provide some association signals but I think they are due to the erroer from the non normality of the phenotype values. Hence I did shapiro-test into it and found that actually, the log normalized values were not normalized. But this lead to toallty lose the power of the study. ( Well this is really disapponting)












